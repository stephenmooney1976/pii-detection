{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5406e4bd-7b15-43d3-a6f2-4ca3131e372a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First, let's install the packages we will need. The following libraries will be used throughout the project:\n",
    "\n",
    "- huggingface_hub\n",
    "- presidio_analyzer\n",
    "- presidio_anonymizer\n",
    "- presidio_image_redactor\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9999fd68-3351-40cb-8142-24f318f2c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt -q\n",
    "# below is a fix for HuggingFace + Tensorflow 2.13+\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3539033-f133-45df-8e4d-d913edf679f3",
   "metadata": {},
   "source": [
    "### We are going to download and use the dslim/bert-base-NER to augment PII detection. \n",
    "\n",
    "_bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd2167b-6a91-4f41-ac8b-bd7a6f8edb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ccf4ac4c6b493e92e1dcdc133a2d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/spm1976/development/pii-analyzer-anonymizer/bert-base-NER'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = 'dslim/bert-base-NER'\n",
    "model_id = repo_id.split('/')[-1]\n",
    "\n",
    "snapshot_download(repo_id=repo_id, local_dir=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903173c2-a72e-4da3-a7aa-6a65f91a3910",
   "metadata": {},
   "source": [
    "### Next we will implement our Presidio anonymizer.\n",
    "\n",
    "_First the base analyzer is created and initialized_\n",
    "_Second we will create a class to extend the base analyzer instantiation_\n",
    "\n",
    "Because Spacy is large, we don't want to download it every time. This code checks to see if it is already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2768746-607a-43e7-8a19-a30a68a5ad25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "  nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "except ModuleNotFoundError:\n",
    "  download(model=\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7f9d2-2976-4d5b-a6f4-74bd24bd2282",
   "metadata": {},
   "source": [
    "This defines the packages that we want to download and the anonymous entries that we want to search for. This can be customized. See https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3073ca1d-7742-4b8a-8ed8-5bb3b5beb5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from typing import List  \n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# load spacy model -> workaround\n",
    "#import os\n",
    "#os.system(\"spacy download en_core_web_lg\")\n",
    "\n",
    "# list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities\n",
    "DEFAULT_ANOYNM_ENTITIES = [\n",
    "    \"CREDIT_CARD\", \n",
    "    \"CRYPTO\",\n",
    "    \"DATE_TIME\",\n",
    "    \"EMAIL_ADDRESS\",\n",
    "    \"IBAN_CODE\",\n",
    "    \"IP_ADDRESS\",\n",
    "    \"NRP\",\n",
    "    \"LOCATION\",\n",
    "    \"PERSON\",\n",
    "    \"PHONE_NUMBER\",\n",
    "    \"MEDICAL_LICENSE\",\n",
    "    \"URL\",\n",
    "    \"ORGANIZATION\",\n",
    "    \"US_SSN\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7a06a-ea92-4d50-9767-7b5337cf6fbc",
   "metadata": {},
   "source": [
    "This is the implementation of our NER EntityRecognizer. More information on this can be found at: https://microsoft.github.io/presidio/analyzer/adding_recognizers/#extending-the-analyzer-for-additional-pii-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd231b5-07cc-45a8-a0c7-ed080fea35af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implement EntityRecognizer class for HuggingFace NER model\n",
    "class TransformerRecognizer(EntityRecognizer):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path=None,\n",
    "        aggregation_strategy='simple',\n",
    "        supported_language='en',\n",
    "        ignore_labels=['0','O','MISC']\n",
    "    ):\n",
    "         # initialize transformers pipeline for given mode or path\n",
    "        self.pipeline = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=model_id_or_path, \n",
    "            aggregation_strategy=aggregation_strategy,\n",
    "            ignore_labels=ignore_labels\n",
    "        )\n",
    "        \n",
    "        # map labels to presidio labels\n",
    "        self.label2presidio = {\n",
    "            \"PER\": \"PERSON\",\n",
    "            \"LOC\": \"LOCATION\",\n",
    "            \"ORG\": \"ORGANIZATION\"\n",
    "        }\n",
    "        \n",
    "        #pass entities from model to parent class\n",
    "        super().__init__(\n",
    "            supported_entities=list(self.label2presidio.values()), \n",
    "            supported_language=supported_language\n",
    "        )\n",
    "        \n",
    "    '''\n",
    "    '''\n",
    "    def load(self):\n",
    "        ''' no loading is required '''\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    def analyze(\n",
    "        self,\n",
    "        text,\n",
    "        entities=None,\n",
    "        nlp_artifacts=None\n",
    "    ):        \n",
    "        predicted_entities = self.pipeline(text)\n",
    "        \n",
    "        results = [ \n",
    "            RecognizerResult(entity_type=self.label2presidio[e['entity_group']], \n",
    "                             start=e['start'], \n",
    "                             end=e['end'], \n",
    "                             score=e['score']) for e in predicted_entities\n",
    "        ]\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24b7ea-60d8-47a5-bee9-a8b789493ae7",
   "metadata": {},
   "source": [
    "In order to detect PII, we are going to use the Presidio AnalyzerEngine, and then register our NER EntityRecognizer into the pipeline. This furthers our capability to detect other PII fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db39a44-03b7-4c1b-afa1-31eec9695da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'bert-base-NER' # directory that we downloaded HuggingFace to above\n",
    "\n",
    "xfmr_recognizer = TransformerRecognizer(model_dir)\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer.registry.add_recognizer(xfmr_recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b258d11-36ad-42d8-9fa4-8e53a7adb0e1",
   "metadata": {},
   "source": [
    "Using the default encoder, this is what ouput from Presidio looks like. It uses generic tags and does not give a nice format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def4cdfb-fb5b-43e5-ba10-e5314650ba5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: PERSON, start: 16, end: 21, score: 0.944421648979187, type: PHONE_NUMBER, start: 46, end: 58, score: 0.75]\n"
     ]
    }
   ],
   "source": [
    "text = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n",
    "\n",
    "analyzer_results = analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "print(analyzer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206f9eea-186c-4b15-b64e-8126c2fac413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: His name is Mr. <PERSON> and his phone number is <PHONE_NUMBER>\n",
      "items:\n",
      "[\n",
      "    {'start': 49, 'end': 63, 'entity_type': 'PHONE_NUMBER', 'text': '<PHONE_NUMBER>', 'operator': 'replace'},\n",
      "    {'start': 16, 'end': 24, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the anonymizer. this is not the extended EntityRecognizer\n",
    "anonymizer_engine = AnonymizerEngine()\n",
    "\n",
    "# create anonymized results\n",
    "anonymized_results = anonymizer_engine.anonymize(\n",
    "    text=text, analyzer_results=analyzer_results\n",
    ")\n",
    "\n",
    "print(anonymized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb1fb70-9a1c-44e2-adc8-b6eeb722da13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}),\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\n",
    "        \"mask\",\n",
    "        {\n",
    "            \"type\": \"mask\",\n",
    "            \"masking_char\": \"*\",\n",
    "            \"chars_to_mask\": 12,\n",
    "            \"from_end\": True,\n",
    "        },\n",
    "    ),\n",
    "    \"US_SSN\": OperatorConfig(\n",
    "        \"mask\",\n",
    "        {\n",
    "            \"type\": \"mask\",\n",
    "            \"masking_char\": \"#\",\n",
    "            \"chars_to_mask\": 11,\n",
    "            \"from_end\": False\n",
    "        }\n",
    "    ),\n",
    "    \"TITLE\": OperatorConfig(\"redact\", {}),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c3674-77fd-477a-85e6-b229b025f1c5",
   "metadata": {},
   "source": [
    "By adding operators to the AnonymizerEngine instance, the output can be customized to produce a more desireable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367d6589-fbcf-49c3-b261-78065ef232b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: His name is Mr. <ANONYMIZED> and his phone number is ************\n",
      "items:\n",
      "[\n",
      "    {'start': 53, 'end': 65, 'entity_type': 'PHONE_NUMBER', 'text': '************', 'operator': 'mask'},\n",
      "    {'start': 16, 'end': 28, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the anonymizer. this is not the extended EntityRecognizer\n",
    "anonymizer_engine = AnonymizerEngine()\n",
    "\n",
    "# create anonymized results\n",
    "anonymized_results = anonymizer_engine.anonymize(\n",
    "    text=text, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(anonymized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811a9da-6438-4c04-8df2-83eab15790d8",
   "metadata": {},
   "source": [
    "This is a longer test of the anonymizer engine with custom operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16aaf4b-0daa-4236-8b21-fb005f5e983e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \n",
      "<ANONYMIZED>, born in <ANONYMIZED>, lives in <ANONYMIZED>, <ANONYMIZED>. \n",
      "He is a software engineer and has a Bachelor's degree in Computer Science from the <ANONYMIZED>. \n",
      "He drives a blue Honda Accord and his driver's license number is <ANONYMIZED>. \n",
      "His social security number is ########### and his phone number is (2************. \n",
      "<ANONYMIZED> enjoys playing basketball and hiking in his free time. \n",
      "He is married to <ANONYMIZED> and they have two children, <ANONYMIZED> and <ANONYMIZED>.\n",
      "He banks at <ANONYMIZED> and his account number is ***********\n",
      "\n",
      "items:\n",
      "[\n",
      "    {'start': 545, 'end': 556, 'entity_type': 'PHONE_NUMBER', 'text': '***********', 'operator': 'mask'},\n",
      "    {'start': 506, 'end': 518, 'entity_type': 'ORGANIZATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 480, 'end': 492, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 463, 'end': 475, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 422, 'end': 434, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 336, 'end': 348, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 319, 'end': 333, 'entity_type': 'PHONE_NUMBER', 'text': '(2************', 'operator': 'mask'},\n",
      "    {'start': 283, 'end': 294, 'entity_type': 'US_SSN', 'text': '###########', 'operator': 'mask'},\n",
      "    {'start': 238, 'end': 250, 'entity_type': 'US_DRIVER_LICENSE', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 158, 'end': 170, 'entity_type': 'ORGANIZATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 60, 'end': 72, 'entity_type': 'LOCATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 46, 'end': 58, 'entity_type': 'LOCATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 23, 'end': 35, 'entity_type': 'DATE_TIME', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 1, 'end': 13, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "John Smith, born in 1987, lives in Seattle, Washington. \n",
    "He is a software engineer and has a Bachelor's degree in Computer Science from the University of Washington. \n",
    "He drives a blue Honda Accord and his driver's license number is A123456789. \n",
    "His social security number is 995-12-2716 and his phone number is (206) 555-1234. \n",
    "John enjoys playing basketball and hiking in his free time. \n",
    "He is married to Sarah Smith and they have two children, Emma and Jake.\n",
    "He banks at JPMC and his account number is 99953153415\n",
    "'''\n",
    "\n",
    "analyzer_results =  analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "anonymized_results = anonymizer_engine.anonymize(\n",
    "    text=text, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(anonymized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b388683-1d61-400e-aeb1-71b855a4a68e",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "Start Redpanda and produce messages from JSON. start_container.bash creates redpanda.env for the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9ed252-b0c7-419c-9df8-cce7f89a1833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "L_NUM_CONTAINERS=3\n",
    "\n",
    "rpk container start -n ${L_NUM_CONTAINERS} | grep export | sed -e 's/^[ \\t]*//' > redpanda.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99066a2e-2f2c-4b91-a97c-6c2dca439195",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m load_dotenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredpanda.env\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\" create producer \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m producer \u001b[38;5;241m=\u001b[39m KafkaProducer(\n\u001b[1;32m     12\u001b[0m     bootstrap_servers \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRPK_BROKERS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m     value_serializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m m: json\u001b[38;5;241m.\u001b[39mdumps(m)\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom-pii-text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_success\u001b[39m(metadata):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/producer/kafka.py:381\u001b[0m, in \u001b[0;36mKafkaProducer.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    378\u001b[0m reporters \u001b[38;5;241m=\u001b[39m [reporter() \u001b[38;5;28;01mfor\u001b[39;00m reporter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_reporters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics \u001b[38;5;241m=\u001b[39m Metrics(metric_config, reporters)\n\u001b[0;32m--> 381\u001b[0m client \u001b[38;5;241m=\u001b[39m KafkaClient(metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics, metric_group_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    382\u001b[0m                      wakeup_timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_block_ms\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    383\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Get auto-discovered version from client if necessary\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/client_async.py:244\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     check_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version_auto_timeout_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_version(timeout\u001b[38;5;241m=\u001b[39mcheck_timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/client_async.py:900\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m--> 900\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoBrokersAvailable()\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_connect(try_node)\n\u001b[1;32m    902\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conns[try_node]\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import os\n",
    "import time\n",
    "\n",
    "\"\"\" read in information about started redpanda environment \"\"\"\n",
    "load_dotenv('redpanda.env')\n",
    "\n",
    "\"\"\" create producer \"\"\"\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = os.environ.get('RPK_BROKERS'),\n",
    "    value_serializer=lambda m: json.dumps(m).encode('ascii')\n",
    ")\n",
    "\n",
    "topic = \"random-pii-text\"\n",
    "\n",
    "def on_success(metadata):\n",
    "  print(f\"Message produced to topic '{metadata.topic}' at offset {metadata.offset}\")\n",
    "\n",
    "def on_error(e):\n",
    "  print(f\"Error sending message: {e}\")\n",
    "\n",
    "\"\"\" read in OpenAI generated PII \"\"\"\n",
    "with open('../data/pii_records.json') as f:\n",
    "  l_json_data = json.load(f)\n",
    "\n",
    "\"\"\" push messages to toic from OpenAI \"\"\"\n",
    "for ii in range(len(l_json_data)):\n",
    "  msg = dict(id=ii, inputs=l_json_data[ii]['inputs'])\n",
    "  future = producer.send(topic, msg)\n",
    "  future.add_callback(on_success)\n",
    "  future.add_errback(on_error)\n",
    "  time.sleep(0.100) # sleep for 1/10 sec to cause a delay.\n",
    "\n",
    "\"\"\" flush and close producer \"\"\"\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa2f79-2c8b-4796-8358-e99ac4315592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!#cd redpanda && ./stop_container.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90844cc0-4031-4549-9571-041a431d2577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
