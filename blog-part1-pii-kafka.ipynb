{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5406e4bd-7b15-43d3-a6f2-4ca3131e372a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First, let's install the packages we will need. The following libraries will be used throughout the project:\n",
    "\n",
    "- huggingface_hub\n",
    "- presidio_analyzer\n",
    "- presidio_anonymizer\n",
    "- presidio_image_redactor\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9999fd68-3351-40cb-8142-24f318f2c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt -q\n",
    "# below is a fix for HuggingFace + Tensorflow 2.13+\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3539033-f133-45df-8e4d-d913edf679f3",
   "metadata": {},
   "source": [
    "### We are going to download and use the dslim/bert-base-NER to augment PII detection. \n",
    "\n",
    "_bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd2167b-6a91-4f41-ac8b-bd7a6f8edb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b44a97f54534fefad938262cae425bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/spm1976/development/pii-analyzer-anonymizer/bert-base-NER'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = 'dslim/bert-base-NER'\n",
    "model_id = repo_id.split('/')[-1]\n",
    "\n",
    "snapshot_download(repo_id=repo_id, local_dir=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903173c2-a72e-4da3-a7aa-6a65f91a3910",
   "metadata": {},
   "source": [
    "### Next we will implement our Presidio anonymizer.\n",
    "\n",
    "_First the base analyzer is created and initialized_\n",
    "_Second we will create a class to extend the base analyzer instantiation_\n",
    "\n",
    "Because Spacy is large, we don't want to download it every time. This code checks to see if it is already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2768746-607a-43e7-8a19-a30a68a5ad25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "  nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "except ModuleNotFoundError:\n",
    "  download(model=\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7f9d2-2976-4d5b-a6f4-74bd24bd2282",
   "metadata": {},
   "source": [
    "This defines the packages that we want to download and the anonymous entries that we want to search for. This can be customized. See https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3073ca1d-7742-4b8a-8ed8-5bb3b5beb5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from typing import List  \n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# load spacy model -> workaround\n",
    "#import os\n",
    "#os.system(\"spacy download en_core_web_lg\")\n",
    "\n",
    "# list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities\n",
    "DEFAULT_ANOYNM_ENTITIES = [\n",
    "    \"CREDIT_CARD\", \n",
    "    \"CRYPTO\",\n",
    "    \"DATE_TIME\",\n",
    "    \"EMAIL_ADDRESS\",\n",
    "    \"IBAN_CODE\",\n",
    "    \"IP_ADDRESS\",\n",
    "    \"NRP\",\n",
    "    \"LOCATION\",\n",
    "    \"PERSON\",\n",
    "    \"PHONE_NUMBER\",\n",
    "    \"MEDICAL_LICENSE\",\n",
    "    \"URL\",\n",
    "    \"ORGANIZATION\",\n",
    "    \"US_SSN\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7a06a-ea92-4d50-9767-7b5337cf6fbc",
   "metadata": {},
   "source": [
    "This is the implementation of our NER EntityRecognizer. More information on this can be found at: https://microsoft.github.io/presidio/analyzer/adding_recognizers/#extending-the-analyzer-for-additional-pii-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd231b5-07cc-45a8-a0c7-ed080fea35af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implement EntityRecognizer class for HuggingFace NER model\n",
    "class TransformerRecognizer(EntityRecognizer):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path=None,\n",
    "        aggregation_strategy='simple',\n",
    "        supported_language='en',\n",
    "        ignore_labels=['0','O','MISC']\n",
    "    ):\n",
    "         # initialize transformers pipeline for given mode or path\n",
    "        self.pipeline = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=model_id_or_path, \n",
    "            aggregation_strategy=aggregation_strategy,\n",
    "            ignore_labels=ignore_labels\n",
    "        )\n",
    "        \n",
    "        # map labels to presidio labels\n",
    "        self.label2presidio = {\n",
    "            \"PER\": \"PERSON\",\n",
    "            \"LOC\": \"LOCATION\",\n",
    "            \"ORG\": \"ORGANIZATION\"\n",
    "        }\n",
    "        \n",
    "        #pass entities from model to parent class\n",
    "        super().__init__(\n",
    "            supported_entities=list(self.label2presidio.values()), \n",
    "            supported_language=supported_language\n",
    "        )\n",
    "        \n",
    "    '''\n",
    "    '''\n",
    "    def load(self):\n",
    "        ''' no loading is required '''\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    def analyze(\n",
    "        self,\n",
    "        text,\n",
    "        entities=None,\n",
    "        nlp_artifacts=None\n",
    "    ):        \n",
    "        predicted_entities = self.pipeline(text)\n",
    "        \n",
    "        results = [ \n",
    "            RecognizerResult(entity_type=self.label2presidio[e['entity_group']], \n",
    "                             start=e['start'], \n",
    "                             end=e['end'], \n",
    "                             score=e['score']) for e in predicted_entities\n",
    "        ]\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24b7ea-60d8-47a5-bee9-a8b789493ae7",
   "metadata": {},
   "source": [
    "In order to detect PII, we are going to use the Presidio AnalyzerEngine, and then register our NER EntityRecognizer into the pipeline. This furthers our capability to detect other PII fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db39a44-03b7-4c1b-afa1-31eec9695da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'bert-base-NER' # directory that we downloaded HuggingFace to above\n",
    "\n",
    "xfmr_recognizer = TransformerRecognizer(model_dir)\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer.registry.add_recognizer(xfmr_recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b258d11-36ad-42d8-9fa4-8e53a7adb0e1",
   "metadata": {},
   "source": [
    "Using the default encoder, this is what ouput from Presidio looks like. It uses generic tags and does not give a nice format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def4cdfb-fb5b-43e5-ba10-e5314650ba5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: PERSON, start: 16, end: 21, score: 0.944421648979187, type: PHONE_NUMBER, start: 46, end: 58, score: 0.75]\n"
     ]
    }
   ],
   "source": [
    "text = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n",
    "\n",
    "analyzer_results = analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "print(analyzer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206f9eea-186c-4b15-b64e-8126c2fac413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: His name is Mr. <PERSON> and his phone number is <PHONE_NUMBER>\n",
      "items:\n",
      "[\n",
      "    {'start': 49, 'end': 63, 'entity_type': 'PHONE_NUMBER', 'text': '<PHONE_NUMBER>', 'operator': 'replace'},\n",
      "    {'start': 16, 'end': 24, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the anonymizer. this is not the extended EntityRecognizer\n",
    "anonymizer_engine = AnonymizerEngine()\n",
    "\n",
    "# create anonymized results\n",
    "anonymized_results = anonymizer_engine.anonymize(\n",
    "    text=text, analyzer_results=analyzer_results\n",
    ")\n",
    "\n",
    "print(anonymized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb1fb70-9a1c-44e2-adc8-b6eeb722da13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}),\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\n",
    "        \"mask\",\n",
    "        {\n",
    "            \"type\": \"mask\",\n",
    "            \"masking_char\": \"*\",\n",
    "            \"chars_to_mask\": 12,\n",
    "            \"from_end\": True,\n",
    "        },\n",
    "    ),\n",
    "    \"US_SSN\": OperatorConfig(\n",
    "        \"mask\",\n",
    "        {\n",
    "            \"type\": \"mask\",\n",
    "            \"masking_char\": \"#\",\n",
    "            \"chars_to_mask\": 11,\n",
    "            \"from_end\": False\n",
    "        }\n",
    "    ),\n",
    "    \"TITLE\": OperatorConfig(\"redact\", {}),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c3674-77fd-477a-85e6-b229b025f1c5",
   "metadata": {},
   "source": [
    "By adding operators to the AnonymizerEngine instance, the output can be customized to produce a more desireable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367d6589-fbcf-49c3-b261-78065ef232b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: His name is Mr. <ANONYMIZED> and his phone number is ************\n",
      "items:\n",
      "[\n",
      "    {'start': 53, 'end': 65, 'entity_type': 'PHONE_NUMBER', 'text': '************', 'operator': 'mask'},\n",
      "    {'start': 16, 'end': 28, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the anonymizer. this is not the extended EntityRecognizer\n",
    "anonymizer_engine = AnonymizerEngine()\n",
    "\n",
    "# create anonymized results\n",
    "anonymized_results = anonymizer_engine.anonymize(\n",
    "    text=text, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(anonymized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811a9da-6438-4c04-8df2-83eab15790d8",
   "metadata": {},
   "source": [
    "This is a longer test of the anonymizer engine with custom operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16aaf4b-0daa-4236-8b21-fb005f5e983e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \n",
      "<ANONYMIZED>, born in <ANONYMIZED>, lives in <ANONYMIZED>, <ANONYMIZED>. \n",
      "He is a software engineer and has a Bachelor's degree in Computer Science from the <ANONYMIZED>. \n",
      "He drives a blue Honda Accord and his driver's license number is <ANONYMIZED>. \n",
      "His social security number is ########### and his phone number is (2************. \n",
      "<ANONYMIZED> enjoys playing basketball and hiking in his free time. \n",
      "He is married to <ANONYMIZED> and they have two children, <ANONYMIZED> and <ANONYMIZED>.\n",
      "He banks at <ANONYMIZED> and his account number is ***********\n",
      "\n",
      "items:\n",
      "[\n",
      "    {'start': 545, 'end': 556, 'entity_type': 'PHONE_NUMBER', 'text': '***********', 'operator': 'mask'},\n",
      "    {'start': 506, 'end': 518, 'entity_type': 'ORGANIZATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 480, 'end': 492, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 463, 'end': 475, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 422, 'end': 434, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 336, 'end': 348, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 319, 'end': 333, 'entity_type': 'PHONE_NUMBER', 'text': '(2************', 'operator': 'mask'},\n",
      "    {'start': 283, 'end': 294, 'entity_type': 'US_SSN', 'text': '###########', 'operator': 'mask'},\n",
      "    {'start': 238, 'end': 250, 'entity_type': 'US_DRIVER_LICENSE', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 158, 'end': 170, 'entity_type': 'ORGANIZATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 60, 'end': 72, 'entity_type': 'LOCATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 46, 'end': 58, 'entity_type': 'LOCATION', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 23, 'end': 35, 'entity_type': 'DATE_TIME', 'text': '<ANONYMIZED>', 'operator': 'replace'},\n",
      "    {'start': 1, 'end': 13, 'entity_type': 'PERSON', 'text': '<ANONYMIZED>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "John Smith, born in 1987, lives in Seattle, Washington. \n",
    "He is a software engineer and has a Bachelor's degree in Computer Science from the University of Washington. \n",
    "He drives a blue Honda Accord and his driver's license number is A123456789. \n",
    "His social security number is 995-12-2716 and his phone number is (206) 555-1234. \n",
    "John enjoys playing basketball and hiking in his free time. \n",
    "He is married to Sarah Smith and they have two children, Emma and Jake.\n",
    "He banks at JPMC and his account number is 99953153415\n",
    "'''\n",
    "\n",
    "analyzer_results =  analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "anonymized_results = anonymizer_engine.anonymize(\n",
    "    text=text, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(anonymized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b388683-1d61-400e-aeb1-71b855a4a68e",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "Start Redpanda and produce messages from JSON. start_container.bash creates redpanda.env for the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9ed252-b0c7-419c-9df8-cce7f89a1833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd redpanda && ./start_container.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99066a2e-2f2c-4b91-a97c-6c2dca439195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message produced to topic 'random-pii-text' at offset 0\n",
      "Message produced to topic 'random-pii-text' at offset 10\n",
      "Message produced to topic 'random-pii-text' at offset 20\n",
      "Message produced to topic 'random-pii-text' at offset 30\n",
      "Message produced to topic 'random-pii-text' at offset 40\n",
      "Message produced to topic 'random-pii-text' at offset 50\n",
      "Message produced to topic 'random-pii-text' at offset 60\n",
      "Message produced to topic 'random-pii-text' at offset 70\n",
      "Message produced to topic 'random-pii-text' at offset 80\n",
      "Message produced to topic 'random-pii-text' at offset 90\n",
      "Message produced to topic 'random-pii-text' at offset 100\n",
      "Message produced to topic 'random-pii-text' at offset 110\n",
      "Message produced to topic 'random-pii-text' at offset 120\n",
      "Message produced to topic 'random-pii-text' at offset 130\n",
      "Message produced to topic 'random-pii-text' at offset 140\n",
      "Message produced to topic 'random-pii-text' at offset 150\n",
      "Message produced to topic 'random-pii-text' at offset 160\n",
      "Message produced to topic 'random-pii-text' at offset 170\n",
      "Message produced to topic 'random-pii-text' at offset 180\n",
      "Message produced to topic 'random-pii-text' at offset 190\n",
      "Message produced to topic 'random-pii-text' at offset 200\n",
      "Message produced to topic 'random-pii-text' at offset 210\n",
      "Message produced to topic 'random-pii-text' at offset 220\n",
      "Message produced to topic 'random-pii-text' at offset 230\n",
      "Message produced to topic 'random-pii-text' at offset 240\n",
      "Message produced to topic 'random-pii-text' at offset 250\n",
      "Message produced to topic 'random-pii-text' at offset 260\n",
      "Message produced to topic 'random-pii-text' at offset 270\n",
      "Message produced to topic 'random-pii-text' at offset 280\n",
      "Message produced to topic 'random-pii-text' at offset 290\n",
      "Message produced to topic 'random-pii-text' at offset 300\n",
      "Message produced to topic 'random-pii-text' at offset 310\n",
      "Message produced to topic 'random-pii-text' at offset 320\n",
      "Message produced to topic 'random-pii-text' at offset 330\n",
      "Message produced to topic 'random-pii-text' at offset 340\n",
      "Message produced to topic 'random-pii-text' at offset 350\n",
      "Message produced to topic 'random-pii-text' at offset 360\n",
      "Message produced to topic 'random-pii-text' at offset 370\n",
      "Message produced to topic 'random-pii-text' at offset 380\n",
      "Message produced to topic 'random-pii-text' at offset 390\n",
      "Message produced to topic 'random-pii-text' at offset 400\n",
      "Message produced to topic 'random-pii-text' at offset 410\n",
      "Message produced to topic 'random-pii-text' at offset 420\n",
      "Message produced to topic 'random-pii-text' at offset 430\n",
      "Message produced to topic 'random-pii-text' at offset 440\n",
      "Message produced to topic 'random-pii-text' at offset 450\n",
      "Message produced to topic 'random-pii-text' at offset 460\n",
      "Message produced to topic 'random-pii-text' at offset 470\n",
      "Message produced to topic 'random-pii-text' at offset 480\n",
      "Message produced to topic 'random-pii-text' at offset 490\n",
      "Message produced to topic 'random-pii-text' at offset 500\n",
      "Message produced to topic 'random-pii-text' at offset 510\n",
      "Message produced to topic 'random-pii-text' at offset 520\n",
      "Message produced to topic 'random-pii-text' at offset 530\n",
      "Message produced to topic 'random-pii-text' at offset 540\n",
      "Message produced to topic 'random-pii-text' at offset 550\n",
      "Message produced to topic 'random-pii-text' at offset 560\n",
      "Message produced to topic 'random-pii-text' at offset 570\n",
      "Message produced to topic 'random-pii-text' at offset 580\n",
      "Message produced to topic 'random-pii-text' at offset 590\n",
      "Message produced to topic 'random-pii-text' at offset 600\n",
      "Message produced to topic 'random-pii-text' at offset 610\n",
      "Message produced to topic 'random-pii-text' at offset 620\n",
      "Message produced to topic 'random-pii-text' at offset 630\n",
      "Message produced to topic 'random-pii-text' at offset 640\n",
      "Message produced to topic 'random-pii-text' at offset 650\n",
      "Message produced to topic 'random-pii-text' at offset 660\n",
      "Message produced to topic 'random-pii-text' at offset 670\n",
      "Message produced to topic 'random-pii-text' at offset 680\n",
      "Message produced to topic 'random-pii-text' at offset 690\n",
      "Message produced to topic 'random-pii-text' at offset 700\n",
      "Message produced to topic 'random-pii-text' at offset 710\n",
      "Message produced to topic 'random-pii-text' at offset 720\n",
      "Message produced to topic 'random-pii-text' at offset 730\n",
      "Message produced to topic 'random-pii-text' at offset 740\n",
      "Message produced to topic 'random-pii-text' at offset 750\n",
      "Message produced to topic 'random-pii-text' at offset 760\n",
      "Message produced to topic 'random-pii-text' at offset 770\n",
      "Message produced to topic 'random-pii-text' at offset 780\n",
      "Message produced to topic 'random-pii-text' at offset 790\n",
      "Message produced to topic 'random-pii-text' at offset 800\n",
      "Message produced to topic 'random-pii-text' at offset 810\n",
      "Message produced to topic 'random-pii-text' at offset 820\n",
      "Message produced to topic 'random-pii-text' at offset 830\n",
      "Message produced to topic 'random-pii-text' at offset 840\n",
      "Message produced to topic 'random-pii-text' at offset 850\n",
      "Message produced to topic 'random-pii-text' at offset 860\n",
      "Message produced to topic 'random-pii-text' at offset 870\n",
      "Message produced to topic 'random-pii-text' at offset 880\n",
      "Message produced to topic 'random-pii-text' at offset 890\n",
      "Message produced to topic 'random-pii-text' at offset 900\n",
      "Message produced to topic 'random-pii-text' at offset 910\n",
      "Message produced to topic 'random-pii-text' at offset 920\n",
      "Message produced to topic 'random-pii-text' at offset 930\n",
      "Message produced to topic 'random-pii-text' at offset 940\n",
      "Message produced to topic 'random-pii-text' at offset 950\n",
      "Message produced to topic 'random-pii-text' at offset 960\n",
      "Message produced to topic 'random-pii-text' at offset 970\n",
      "Message produced to topic 'random-pii-text' at offset 980\n",
      "Message produced to topic 'random-pii-text' at offset 990\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import os\n",
    "\n",
    "\"\"\" read in information about started redpanda environment \"\"\"\n",
    "load_dotenv('redpanda/redpanda.env', override=True)\n",
    "\n",
    "\"\"\" create producer \"\"\"\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = os.environ.get('RPK_BROKERS'),\n",
    "    value_serializer=lambda m: json.dumps(m).encode('ascii')\n",
    ")\n",
    "\n",
    "topic = \"random-pii-text\"\n",
    "\n",
    "def on_success(metadata):\n",
    "  if metadata.offset % 10 == 0: \n",
    "    print(f\"Message produced to topic '{metadata.topic}' at offset {metadata.offset}\")\n",
    "\n",
    "def on_error(e):\n",
    "  print(f\"Error sending message: {e}\")\n",
    "\n",
    "\"\"\" read in OpenAI generated PII \"\"\"\n",
    "with open('data/pii_records.json') as f:\n",
    "  l_json_data = json.load(f)\n",
    "\n",
    "\"\"\" push messages to topic from OpenAI \"\"\"\n",
    "for ii in range(len(l_json_data)):\n",
    "  msg = {'id': ii, 'inputs': l_json_data[ii]['inputs']}\n",
    "  future = producer.send(topic, msg)\n",
    "  future.add_callback(on_success)\n",
    "  future.add_errback(on_error)\n",
    "\n",
    "\n",
    "\"\"\" flush and close producer \"\"\"\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76626a6-f1d4-4aaa-b009-319ac9ba1ccb",
   "metadata": {},
   "source": [
    "Consuming these messages is just as straight-forward. In this simple consumer I will use the AnalyzerResults generated from each message and do the anonymization.\n",
    "\n",
    "A lot of this code is repeated from previous sections. I condensed it here for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59aed0a6-44c1-426b-873f-e41b49b6f783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'bert-base-NER' # directory that we downloaded HuggingFace to above\n",
    "\n",
    "xfmr_recognizer = TransformerRecognizer(model_dir)\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer.registry.add_recognizer(xfmr_recognizer)\n",
    "\n",
    "anonymizer_engine = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990b1bfd-f892-486a-aa10-9e6c61969bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}),\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\n",
    "        \"mask\",\n",
    "        {\n",
    "            \"type\": \"mask\",\n",
    "            \"masking_char\": \"*\",\n",
    "            \"chars_to_mask\": 12,\n",
    "            \"from_end\": True,\n",
    "        },\n",
    "    ),\n",
    "    \"US_SSN\": OperatorConfig(\n",
    "        \"mask\",\n",
    "        {\n",
    "            \"type\": \"mask\",\n",
    "            \"masking_char\": \"#\",\n",
    "            \"chars_to_mask\": 11,\n",
    "            \"from_end\": False\n",
    "        }\n",
    "    ),\n",
    "    \"TITLE\": OperatorConfig(\"redact\", {}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3903b7c6-28d8-4625-82c6-688130e8bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "load_dotenv('redpanda/redpanda.env', override=True)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "  bootstrap_servers=os.environ.get('RPK_BROKERS'),\n",
    "  group_id=\"demo-group\",\n",
    "  auto_offset_reset=\"earliest\",\n",
    "  enable_auto_commit=False,\n",
    "  consumer_timeout_ms=1000,\n",
    "  value_deserializer=lambda m: json.loads(m.decode('ascii'))\n",
    ")\n",
    "\n",
    "topic = \"random-pii-text\"\n",
    "consumer.subscribe(topic)\n",
    "\n",
    "anonymized_json = list()\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        topic_info = f\"topic: {message.partition}|{message.offset})\"\n",
    "        message_info = f\"key: {message.key}, {message.value}\"\n",
    "\n",
    "        original_json = message.value\n",
    "        original_text = original_json['inputs']\n",
    "\n",
    "        analyzer_results = analyzer.analyze(text=original_text, language=\"en\")\n",
    "\n",
    "        anonymized_text = anonymizer_engine.anonymize(text=original_text,\n",
    "                                                      analyzer_results=analyzer_results,\n",
    "                                                      operators=operators)\n",
    "\n",
    "        original_json['inputs_anon'] = anonymized_text.text\n",
    "\n",
    "\n",
    "        anonymized_json.append(original_json)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while consuming messages: {e}\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c7b31-ba48-4517-beea-77d32fe20f3e",
   "metadata": {},
   "source": [
    "Here is an example of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08becb33-145b-42a7-9a67-49339f5353de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet Mary Johnson, a 34-year-old software engineer from Chicago, Illinois. Mary has been working in the industry for 11 years and currently works for a large tech company. She recently moved to the Bay Area and is enjoying the new lifestyle. Mary is a big fan of outdoor activities such as camping and hiking, and she loves to travel around the world. She’s also an avid reader and enjoys cooking on the weekends. Mary’s social security number is 847-51-6329 and her driver’s license number is L934-908-1228.\n",
      "Meet George Bailey, a 25-year-old software engineer from Chicago, Illinois. George enjoys playing video games, reading fantasy novels, and exploring the outdoors. He recently moved to Seattle, Washington and is excited to explore the city. George's Social Security number is 872-32-9087 and his driver's license number is C188815. George's email address is georgebailey@example.com and his phone number is (206) 555-0130.\n",
      "Alice Johnson just moved to the city and is looking for a new job. She recently updated her driver's license with her new address, which is 123 Main Street, Anytown, USA. Her social security number is 789-456-1234 and her date of birth is April 10th, 1985. Alice also recently applied for a new credit card with her new address, and the card number is 4242-4242-4242-4242.\n",
      "Meet George Harris, a 21-year-old from San Francisco, California. George is a student at the University of California, Berkeley, studying computer science. He has a driver's license number of GX6A9-2B5F-42Y1 and a social security number of 890-21-9456. George's phone number is (415) 567-3490 and his email address is georgeh@example.com. He is passionate about programming and loves to play video games in his spare time.\n",
      "Hello, my name is Paulina Bautista. I'm a 25-year-old student from Miami, Florida. I recently graduated from Florida State University with a degree in Business Administration. I'm currently looking for a job in the finance sector. I'm also interested in learning more about data privacy and security, as I'm sure many of my peers are. I'm excited to see what the future holds for me!\n"
     ]
    }
   ],
   "source": [
    "first5 = anonymized_json[:5]\n",
    "for z in first5:\n",
    "  print(z['inputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c8eb9-3f99-4fd9-acb0-30a0d07bdbe2",
   "metadata": {},
   "source": [
    "Here is an example of the anonymized corresponding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92d516ff-e423-44f9-9196-a43b33d45d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet <ANONYMIZED>, a <ANONYMIZED> software engineer from <ANONYMIZED>, <ANONYMIZED>. <ANONYMIZED> has been working in the industry for <ANONYMIZED> and currently works for a large tech company. She recently moved to <ANONYMIZED> and is enjoying the new lifestyle. <ANONYMIZED> is a big fan of outdoor activities such as camping and hiking, and she loves to travel around the world. She’s also an avid reader and enjoys cooking on <ANONYMIZED>. <ANONYMIZED>’s social security number is ########### and her driver’s license number is <ANONYMIZED>-908-1228.\n",
      "Meet <ANONYMIZED>, a <ANONYMIZED> software engineer from <ANONYMIZED>, <ANONYMIZED>. <ANONYMIZED> enjoys playing video games, reading fantasy novels, and exploring the outdoors. He recently moved to <ANONYMIZED>, <ANONYMIZED> and is excited to explore the city. <ANONYMIZED>'s Social Security number is ########### and his driver's license number is <ANONYMIZED>. <ANONYMIZED>'s email address is <ANONYMIZED> and his phone number is (2************.\n",
      "<ANONYMIZED> just moved to the city and is looking for a new job. She recently updated her driver's license with her new address, which is 123 <ANONYMIZED>, <ANONYMIZED>, <ANONYMIZED>. Her social security number is ************ and her date of birth is <ANONYMIZED>. <ANONYMIZED> also recently applied for a new credit card with her new address, and the card number is <ANONYMIZED>.\n",
      "Meet <ANONYMIZED>, a <ANONYMIZED> from <ANONYMIZED>, <ANONYMIZED>. <ANONYMIZED> is a student at the <ANONYMIZED>, <ANONYMIZED>, studying computer science. He has a driver's license number of GX6A9-2B5F-42Y1 and a social security number of ###########. <ANONYMIZED>'s phone number is (4************ and his email address is <ANONYMIZED>. He is passionate about programming and loves to play video games in his spare time.\n",
      "Hello, my name is <ANONYMIZED>. I'm a <ANONYMIZED> student from <ANONYMIZED>, <ANONYMIZED>. I recently graduated from <ANONYMIZED> with a degree in Business Administration. I'm currently looking for a job in the finance sector. I'm also interested in learning more about data privacy and security, as I'm sure many of my peers are. I'm excited to see what the future holds for me!\n"
     ]
    }
   ],
   "source": [
    "for z in first5:\n",
    "  print(z['inputs_anon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361110a-bfc7-4380-8f1a-fad4948952fc",
   "metadata": {},
   "source": [
    "Shut down Redpanda and clean up Docker resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73fa2f79-2c8b-4796-8358-e99ac4315592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping node 0\n",
      "Stopping node 1\n",
      "Stopping node 2\n",
      "Removed container rp-node-0 (node 0)\n",
      "Removed container rp-node-2 (node 2)\n",
      "Removed container rp-node-1 (node 1)\n",
      "Deleted cluster data.\n"
     ]
    }
   ],
   "source": [
    "!cd redpanda && ./stop_container.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c50d4-c4fe-407d-ac9d-9c65677890ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
